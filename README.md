
# ğŸ§ âœ¨ Implementing the LoRA Paper with TinyLlama ğŸ¦™ğŸ’¡

Welcome to my implementation of the ğŸ”¬ **LoRA (Low-Rank Adaptation)** paper, applied to a real-world fine-tuning task using ğŸ¦™ `TinyLlama-1.1B-Chat`! This project showcases **parameter-efficient fine-tuning** on the ğŸ“ **GSM8K** dataset (grade-school math questions) using 4-bit quantization, Hugging Face ğŸ¤— tools, and LoRA magic ğŸ§ª.

> âœ… This repo contains a clean and practical implementation of the **LoRA paper** (paper included in repo ğŸ“„) for small-scale large language model fine-tuning!

---

## ğŸš€ Project Highlights

âœ¨ **Model:** `TinyLlama/TinyLlama-1.1B-Chat`  
ğŸ“š **Dataset:** `openai/gsm8k` (subsampled)  
ğŸ”§ **Techniques Used:**
- ğŸ§  **LoRA (Low-Rank Adaptation)** â€” lightweight, modular fine-tuning!
- ğŸ§® **BitsAndBytes 4-bit quantization** â€” memory-efficient training
- ğŸ§° **Hugging Face Transformers, PEFT, Datasets** â€” the modern NLP toolkit
- ğŸ§ª **Causal Language Modeling** â€” solving math problems via generation!

---

## ğŸ““ Notebook: Whatâ€™s Inside

- ğŸ“¥ Model loading & quantization
- ğŸ”— Injecting LoRA adapters into attention layers
- ğŸ“Š Dataset loading + tokenization
- ğŸ‹ï¸â€â™‚ï¸ Fine-tuning on GSM8K
- ğŸ§ª Evaluation using exact-match accuracy
- ğŸ“‰ Logging and results output

---

## ğŸ—ƒï¸ Repo Structure

```
ğŸ“ project-root/
â”œâ”€â”€ Finetuning_LORA.ipynb       # Main training notebook ğŸ““
â”œâ”€â”€ README.md                   # You're reading it! ğŸ˜
â”œâ”€â”€ LoRA-Paper.pdf              # The original LoRA paper ğŸ“„
â””â”€â”€ results/                    # (Optional) Sample predictions & logs ğŸ“Š
```

---

## ğŸ“¦ Installation & Requirements

```bash
pip install transformers datasets peft bitsandbytes
```

âœ… Tested on Google Colab with a T4 GPU.

---

## ğŸ’¬ Credits & Notes

This is a **faithful implementation of the LoRA paper**, aimed at understanding how **parameter-efficient fine-tuning** works in practice.

ğŸ“Œ **Goal:** Learn-by-doing â†’ replicate academic papers â†’ scale up ğŸš€  
ğŸ“ Paper is included in this repo for reference.

---

## ğŸ¤– Stay tuned

More experiments coming soon:
- ğŸ“ Chain-of-Thought reasoning on GSM8K
- ğŸ“ˆ Accuracy benchmarking across models

If you liked this, feel free to â­ï¸ the repo or fork for your own LoRA adventures!
