
# ğŸ§ âœ¨ Implementing the LoRA Paper with TinyLlama ğŸ¦™ğŸ’¡

Welcome to my implementation of the ğŸ”¬ **LoRA (Low-Rank Adaptation)** paper, applied to a real-world fine-tuning task using ğŸ¦™ `TinyLlama-1.1B-Chat`! This project showcases **parameter-efficient fine-tuning** on the ğŸ“ **GSM8K** dataset (grade-school math questions) using 4-bit quantization, Hugging Face ğŸ¤— tools, and LoRA magic ğŸ§ª.

> âœ… This repo contains a clean and practical implementation of the **LoRA paper** (paper included in repo ğŸ“„) for small-scale large language model fine-tuning!

---

## ğŸš€ Project Highlights

âœ¨ **Model:** `TinyLlama/TinyLlama-1.1B-Chat`  
ğŸ“š **Dataset:** `openai/gsm8k` (subsampled)  
ğŸ”§ **Techniques Used:**
- ğŸ§  **LoRA (Low-Rank Adaptation)** â€” lightweight, modular fine-tuning!
- ğŸ§® **BitsAndBytes 4-bit quantization** â€” memory-efficient training
- ğŸ§° **Hugging Face Transformers, PEFT, Datasets** â€” the modern NLP toolkit
- ğŸ§ª **Causal Language Modeling** â€” solving math problems via generation!

---

## ğŸ““ Notebook: Whatâ€™s Inside

- ğŸ“¥ Model loading & quantization
- ğŸ”— Injecting LoRA adapters into attention layers
- ğŸ“Š Dataset loading + tokenization
- ğŸ‹ï¸â€â™‚ï¸ Fine-tuning on GSM8K
- ğŸ§ª Evaluation using exact-match accuracy
- ğŸ“‰ Logging and results output

---

## ğŸ—ƒï¸ Repo Structure

```
ğŸ“ project-root/
â”œâ”€â”€ Finetuning_LORA.ipynb       # Main training notebook ğŸ““
â”œâ”€â”€ README.md                   # You're reading it! ğŸ˜
â”œâ”€â”€ LoRA-Paper.pdf              # The original LoRA paper ğŸ“„

